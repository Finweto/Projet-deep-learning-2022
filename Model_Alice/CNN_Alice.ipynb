{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN Alice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports et Constantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0UfR5537lvdG"
      },
      "outputs": [],
      "source": [
        "############################################################## IMPORTS ET CONSTANTES ##############################################################\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import csv\n",
        "import random\n",
        "import zipfile\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import * \n",
        "from tensorflow.keras import regularizers\n",
        "import matplotlib\n",
        "from keras.models import Sequential\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "#uploaded = files.upload()\n",
        "\n",
        "\n",
        "training_size = 20\n",
        "training_longueur = 975020\n",
        "nombre_etapes_csv = 10\n",
        "testing_size = 10\n",
        "nb_pixels_images = 97500\n",
        "largeur_image = 390\n",
        "longueur_image = 250\n",
        "nb_channel = 1 \n",
        "n_classes = 10\n",
        "\n",
        "\n",
        "############################################################## PARSING ##############################################################\n",
        "def parse_data_from_input(filename, training_size, training_longueur, largeur_image, longueur_image, nb_pixels_images, n_classes):\n",
        "  with open(filename) as file:\n",
        "    lab = []\n",
        "    img = []\n",
        "  \n",
        "    CSVData = open(filename)\n",
        "    array = np.loadtxt(CSVData, delimiter=\",\")\n",
        "\n",
        "    # récupération des labels de training\n",
        "    for j in range(nb_pixels_images, training_longueur, (nb_pixels_images+2)):\n",
        "        lab.append(array[j])\n",
        "    labels = np.array(lab).flatten()\n",
        "    \n",
        "    # récupération des images de training\n",
        "    for i in range(nombre_etapes_csv):\n",
        "        for bigLine in range(training_size):\n",
        "          for line in range(i*(nb_pixels_images+2), (i*(nb_pixels_images+2))+nb_pixels_images, 1):\n",
        "            img.append((array[line][bigLine]))\n",
        "        img2 = np.expand_dims(img, axis=0)\n",
        "    images = img2.reshape(training_size*nombre_etapes_csv, largeur_image, longueur_image)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "############################################################## DATA AUGMENTATION ##############################################################\n",
        "def simple_train_val_generators(training_images, training_labels, training_size, largeur_image, longueur_image, nb_channel, n_classes):\n",
        "  training_images = training_images.reshape(training_size*(n_classes), largeur_image, longueur_image, nb_channel) \n",
        "  \n",
        "  # data augmentation\n",
        "  train_datagen = ImageDataGenerator(\n",
        "       rescale = 1./255.,\n",
        "       horizontal_flip=True,\n",
        "       zoom_range=0.3)\n",
        "  train_generator = train_datagen.flow(x=training_images,\n",
        "                                       y=training_labels,\n",
        "                                       batch_size=10) \n",
        "\n",
        "  return train_generator\n",
        "\n",
        "\n",
        "############################################################## ARCHITECTURES CNNs ##############################################################\n",
        "class myCallback(tf.keras.callbacks.Callback): \n",
        "  # early stoppingquand la précision sur l'ensmble d'entrainement dépasse 90% pour éviter le sur-apprentissage\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.82):\n",
        "      self.model.stop_training = True\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def create_model(n_classes, largeur_image, longueur_image, nb_channel):\n",
        "  model = tf.keras.models.Sequential([\n",
        "    # features extraction\n",
        "    tf.keras.layers.Conv2D(250, (2,2), activation='relu', input_shape = (largeur_image, longueur_image, nb_channel)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.45),\n",
        "    tf.keras.layers.Conv2D(150, (2,2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "    tf.keras.layers.Dropout(0.45),\n",
        "    tf.keras.layers.Conv2D(100, (2,2), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
        "    # classification\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.45),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(n_classes+1, activation='softmax')\n",
        "  ])\n",
        "  model.summary()\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def run_model(train_generator, n_classes, largeur_image, longueur_image, nb_channel, quick):\n",
        "  model = create_model(n_classes, largeur_image, longueur_image, nb_channel)\n",
        "  call = myCallback()\n",
        " \n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    print(logs.get('accuracy'))\n",
        "\n",
        "  # Si quick = 0 on n'affche pas le détail de l'apprentissage\n",
        "  history = model.fit(train_generator,\n",
        "                        epochs=100, \n",
        "                        verbose=quick,\n",
        "                        callbacks=[call])\n",
        "  return model\n",
        "\n",
        "\n",
        "############################################################## PREDICTIONS ET EVALUATIONS #####################################################\n",
        "def prediction_1_vecteur(filename, largeur_image, longueur_image, nb_channel, indice_li, indice_co, nb_pixels_images, model, n_classes, quick):\n",
        "  CSVData = open(filename)\n",
        "  testing_array = np.loadtxt(CSVData, delimiter=\",\")\n",
        "  img = []\n",
        "  correct = 0\n",
        "  \n",
        "  # récupération de l'image à prédire\n",
        "  for line in range(nb_pixels_images):\n",
        "    img.append((testing_array[((nb_pixels_images+2)*indice_li)+line][indice_co]))\n",
        "  img_to_pred = np.expand_dims(img, axis=0)\n",
        "  img_to_pred = img_to_pred.reshape(-1,largeur_image, longueur_image, nb_channel)\n",
        "  img_to_pred/=255.\n",
        "\n",
        "  # récupération du label et du numéro de cas de l'image à prédire\n",
        "  label = testing_array[((nb_pixels_images+2)*indice_li)+nb_pixels_images][indice_co]\n",
        "  numero_cas_img_to_pred = testing_array[((nb_pixels_images+2)*indice_li)+nb_pixels_images+1][indice_co]\n",
        "  predictions = model.predict(img_to_pred)\n",
        "  classe = np.argmax(predictions, axis = 1)\n",
        "\n",
        "  if(classe==label):\n",
        "    correct=1\n",
        "  else:\n",
        "    correct=0\n",
        "      \n",
        "  if(quick==1):\n",
        "    print(\"\\nPREDICTION DES CARACTERISTIQUES A PARTIR D'1 IMAGE : \")\n",
        "    print(\"numéro de l'image : \", label, \"_\", numero_cas_img_to_pred)\n",
        "    print(\"label à prédire : \", label)\n",
        "    print(\"classe prédite : \", classe)\n",
        "    if(label==classe):\n",
        "      print(\"La prédiction est correcte !\")\n",
        "      if (classe==1):\n",
        "        caracteristiques = \"amborella, bords lisses, phyllotaxie alternée, feuilles simples, non ligneux\"\n",
        "      elif (classe==2):\n",
        "        caracteristiques = \"castanea, bords dentés, phyllotaxie alternée, feuilles simples, ligneux\"\n",
        "      elif (classe==3):\n",
        "        caracteristiques = \"convolvulaceae, bords lisses, phyllotaxie alternée, feuilles simples, non ligneux\"\n",
        "      elif (classe==4):\n",
        "        caracteristiques = \"desmodium, bords lisses, phyllotaxie alternée, feuilles composées, non ligneux\"\n",
        "      elif (classe==5):\n",
        "        caracteristiques = \"eugenia, bords lisses, phyllotaxie opposée, feuilles simples, ligneux\"\n",
        "      elif (classe==6):\n",
        "        caracteristiques = \"laurus, bords lisses, phyllotaxie opposée, feuilles simples, ligneux\"\n",
        "      elif (classe==7):\n",
        "        caracteristiques = \"litsea, bords lisses, phyllotaxie alternée, feuilles simples, ligneux\"\n",
        "      elif (classe==8):\n",
        "        caracteristiques = \"magnolia, bords lisses, phyllotaxie alternée, feuilles simples, ligneux\"\n",
        "      elif (classe==9):\n",
        "        caracteristiques = \"monimiaceae, bords lisses, phyllotaxie oppposée, feuilles simples, ligneux\"\n",
        "      elif (classe==10):\n",
        "        caracteristiques = \"rubus, bords dentés, phyllotaxie alternée, feuilles composées, ligneux\"\n",
        "      elif (classe==11):\n",
        "        caracteristiques = \"ulmus, bords dentés, phyllotaxie alternée, feuilles simples, ligneux\"\n",
        "      print(\"caractéristiques prédites : \", caracteristiques)\n",
        "    else:\n",
        "      print(\"Malheureusement la prédiction est incorrecte...\")\n",
        "\n",
        "  return numero_cas_img_to_pred, label, classe, img_to_pred, correct\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def prediction_dataset(filename, testing_size, nb_pixels_images, model, n_classes, quick):\n",
        "  if(quick==0):\n",
        "    print(\"\\nPREDICTION DES CARACTERISTIQUES A PARTIR DE L'ENSEMBLE DE TEST : \")  \n",
        "  with open(filename) as file:\n",
        "    total_good_predictions = 0\n",
        "    good_predictions_per_classe = np.zeros(n_classes)\n",
        "    bad_predictions_name = []\n",
        "    bad_predictions_img = []\n",
        "    realite = np.zeros(n_classes*testing_size)\n",
        "    prediction = np.zeros(n_classes*testing_size)\n",
        "\n",
        "    realite = realite.reshape(1,n_classes*testing_size)\n",
        "    prediction = prediction.reshape(1,n_classes*testing_size)\n",
        "\n",
        "    # parcours du dataset de test\n",
        "    for indice_li in range (n_classes):                                                     \n",
        "      for indice_co in range(testing_size):\n",
        "        numero_cas_img_to_pred, label, classe, img_to_pred, correct = prediction_1_vecteur(filename, largeur_image, longueur_image, nb_channel, indice_li, indice_co, nb_pixels_images, model, n_classes, quick)\n",
        "        realite[0][indice_li*testing_size+indice_co] = float(label)\n",
        "        prediction[0][indice_li*testing_size+indice_co] = float(classe)\n",
        "        if(correct==1):\n",
        "          total_good_predictions+=1\n",
        "          good_predictions_per_classe[indice_li]+=1\n",
        "        else:\n",
        "          bad_case = str(label) + '_' + str(numero_cas_img_to_pred)\n",
        "          bad_predictions_name.append(bad_case)\n",
        "          bad_predictions_img.append(img_to_pred)\n",
        "          \n",
        "    # création de la matrice de confusion\n",
        "    realite = realite.reshape(n_classes*testing_size,)\n",
        "    prediction = prediction.reshape(n_classes*testing_size,)\n",
        "    y_actu = pd.Series(realite, name='Reality')\n",
        "    y_pred = pd.Series(prediction, name='Predicted')\n",
        "    df_confusion = pd.crosstab(y_actu, y_pred)\n",
        "    mae = tf.keras.metrics.mean_absolute_error(realite, prediction).numpy()\n",
        "\n",
        "    # évaluation des performances en fonction des classes à prédire\n",
        "    pourcentage_good_predictions = (total_good_predictions * 100) / (n_classes*testing_size)\n",
        "    if(quick==quick): # avec affichage\n",
        "      print(\"Le pourcentage de good prédictions sur le testing set est de : \", pourcentage_good_predictions, \"%. \\n -------------------------------------------------------------------\")\n",
        "      for i in range(n_classes):\n",
        "        print(\"Le pourcentage de good prédictions pour la classe \", str(i+1), \" est de : \", (good_predictions_per_classe[i]*100)/testing_size, \"%.\")     \n",
        "      print(df_confusion)\n",
        "      print(len(bad_predictions_name), \"grille(s) mal classée(s) :\")\n",
        "      for i in range(len(bad_predictions_name)):\n",
        "        print(\"  \", bad_predictions_name[i])\n",
        "      print(\"MAE : \", mae)\n",
        "\n",
        "  return good_predictions_per_classe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Programme principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtXhB4gBl9K3",
        "outputId": "68757f88-773b-407f-98c2-17045d2fea75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-------------\n",
            "BONJOUR, JE SUIS GULDURNet  ! \n",
            "JE SUIS UN cnn QUI VA ESTIMER LES CARACTERISTIQUES DE PLANTES A PARTIR D'IMAGES.\n",
            "-------------\n",
            "\n",
            "\n",
            "IMPORTS ...\n",
            "\n",
            "LIENS AVEC LES DATASETS\n",
            "\n",
            "PARSING ... \n",
            "Training images has shape: (200, 390, 250) and dtype: float64\n",
            "Training labels has shape: (200,) and dtype: float64\n",
            "\n",
            "DATA AUGMENTATION\n",
            "Images of training generator have shape: (200, 390, 250, 1)\n",
            "Labels of training generator have shape: (200,)\n",
            "\n",
            "CNN\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_14 (Conv2D)          (None, 389, 249, 250)     1250      \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  (None, 195, 125, 250)    0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 195, 125, 250)     0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 194, 124, 150)     150150    \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  (None, 97, 62, 150)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 97, 62, 150)       0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 96, 61, 100)       60100     \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  (None, 48, 31, 100)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 148800)            0         \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 148800)            0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               76186112  \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 11)                2827      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 76,531,767\n",
            "Trainable params: 76,531,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            " 6/20 [========>.....................] - ETA: 2s - loss: 7.1698 - accuracy: 0.1000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0788s vs `on_train_batch_end` time: 0.1272s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 5s 202ms/step - loss: 3.8258 - accuracy: 0.0950\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 2.3442 - accuracy: 0.1000\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 2.3253 - accuracy: 0.0850\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.3160 - accuracy: 0.1050\n",
            "Epoch 5/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.3283 - accuracy: 0.1200\n",
            "Epoch 6/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.2910 - accuracy: 0.1100\n",
            "Epoch 7/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 2.2735 - accuracy: 0.1050\n",
            "Epoch 8/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 2.2586 - accuracy: 0.1450\n",
            "Epoch 9/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 2.3453 - accuracy: 0.1000\n",
            "Epoch 10/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 2.3265 - accuracy: 0.0850\n",
            "Epoch 11/100\n",
            "20/20 [==============================] - 4s 205ms/step - loss: 2.3224 - accuracy: 0.1150\n",
            "Epoch 12/100\n",
            "20/20 [==============================] - 4s 205ms/step - loss: 2.3173 - accuracy: 0.0950\n",
            "Epoch 13/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 2.2964 - accuracy: 0.1150\n",
            "Epoch 14/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.2619 - accuracy: 0.1450\n",
            "Epoch 15/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 2.2127 - accuracy: 0.1550\n",
            "Epoch 16/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.1861 - accuracy: 0.2100\n",
            "Epoch 17/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.3169 - accuracy: 0.0950\n",
            "Epoch 18/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.3077 - accuracy: 0.1150\n",
            "Epoch 19/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 2.2099 - accuracy: 0.1700\n",
            "Epoch 20/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 2.1420 - accuracy: 0.1950\n",
            "Epoch 21/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.2869 - accuracy: 0.1200\n",
            "Epoch 22/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.1686 - accuracy: 0.1900\n",
            "Epoch 23/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.1098 - accuracy: 0.2200\n",
            "Epoch 24/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.0631 - accuracy: 0.2450\n",
            "Epoch 25/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.0058 - accuracy: 0.2700\n",
            "Epoch 26/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.0225 - accuracy: 0.2700\n",
            "Epoch 27/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 2.0258 - accuracy: 0.2650\n",
            "Epoch 28/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 1.9732 - accuracy: 0.3000\n",
            "Epoch 29/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.9963 - accuracy: 0.2550\n",
            "Epoch 30/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.8403 - accuracy: 0.3150\n",
            "Epoch 31/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 1.9138 - accuracy: 0.2700\n",
            "Epoch 32/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.8212 - accuracy: 0.3100\n",
            "Epoch 33/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.8824 - accuracy: 0.3250\n",
            "Epoch 34/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.7744 - accuracy: 0.3400\n",
            "Epoch 35/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.9052 - accuracy: 0.3000\n",
            "Epoch 36/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.8358 - accuracy: 0.3600\n",
            "Epoch 37/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.7207 - accuracy: 0.3850\n",
            "Epoch 38/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.7277 - accuracy: 0.3750\n",
            "Epoch 39/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.7291 - accuracy: 0.3650\n",
            "Epoch 40/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.6261 - accuracy: 0.4100\n",
            "Epoch 41/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.6126 - accuracy: 0.4300\n",
            "Epoch 42/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.5916 - accuracy: 0.4600\n",
            "Epoch 43/100\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 1.5235 - accuracy: 0.4650\n",
            "Epoch 44/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.5550 - accuracy: 0.4400\n",
            "Epoch 45/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.4803 - accuracy: 0.4950\n",
            "Epoch 46/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.5179 - accuracy: 0.4300\n",
            "Epoch 47/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.5209 - accuracy: 0.4750\n",
            "Epoch 48/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.4939 - accuracy: 0.4400\n",
            "Epoch 49/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.3965 - accuracy: 0.5300\n",
            "Epoch 50/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.3726 - accuracy: 0.4900\n",
            "Epoch 51/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.3069 - accuracy: 0.5100\n",
            "Epoch 52/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.4060 - accuracy: 0.5400\n",
            "Epoch 53/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.4518 - accuracy: 0.4900\n",
            "Epoch 54/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.2195 - accuracy: 0.5800\n",
            "Epoch 55/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.1178 - accuracy: 0.5750\n",
            "Epoch 56/100\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 1.2083 - accuracy: 0.5750\n",
            "Epoch 57/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.0649 - accuracy: 0.6100\n",
            "Epoch 58/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.1799 - accuracy: 0.5950\n",
            "Epoch 59/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.1111 - accuracy: 0.6000\n",
            "Epoch 60/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.1671 - accuracy: 0.5900\n",
            "Epoch 61/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.9647 - accuracy: 0.6500\n",
            "Epoch 62/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.0505 - accuracy: 0.6050\n",
            "Epoch 63/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.9426 - accuracy: 0.6800\n",
            "Epoch 64/100\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 1.0248 - accuracy: 0.6450\n",
            "Epoch 65/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.8325 - accuracy: 0.7350\n",
            "Epoch 66/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.0022 - accuracy: 0.6950\n",
            "Epoch 67/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.8136 - accuracy: 0.6950\n",
            "Epoch 68/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 0.9666 - accuracy: 0.6550\n",
            "Epoch 69/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 1.1184 - accuracy: 0.5700\n",
            "Epoch 70/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 1.0474 - accuracy: 0.6550\n",
            "Epoch 71/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.8731 - accuracy: 0.7250\n",
            "Epoch 72/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.8734 - accuracy: 0.7050\n",
            "Epoch 73/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.7881 - accuracy: 0.7200\n",
            "Epoch 74/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.7034 - accuracy: 0.7550\n",
            "Epoch 75/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.8571 - accuracy: 0.6950\n",
            "Epoch 76/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.7258 - accuracy: 0.7400\n",
            "Epoch 77/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.6850 - accuracy: 0.7850\n",
            "Epoch 78/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.6709 - accuracy: 0.7600\n",
            "Epoch 79/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.5218 - accuracy: 0.8350\n",
            "Epoch 80/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.7574 - accuracy: 0.7100\n",
            "Epoch 81/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.6093 - accuracy: 0.7850\n",
            "Epoch 82/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.6016 - accuracy: 0.7900\n",
            "Epoch 83/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.5460 - accuracy: 0.8350\n",
            "Epoch 84/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.6030 - accuracy: 0.8250\n",
            "Epoch 85/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.6776 - accuracy: 0.8200\n",
            "Epoch 86/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.7674 - accuracy: 0.7450\n",
            "Epoch 87/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.5475 - accuracy: 0.8250\n",
            "Epoch 88/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.6460 - accuracy: 0.7900\n",
            "Epoch 89/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.5741 - accuracy: 0.7950\n",
            "Epoch 90/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.5414 - accuracy: 0.8000\n",
            "Epoch 91/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.5324 - accuracy: 0.8200\n",
            "Epoch 92/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.4801 - accuracy: 0.8400\n",
            "Epoch 93/100\n",
            "20/20 [==============================] - 4s 202ms/step - loss: 0.5149 - accuracy: 0.8250\n",
            "Epoch 94/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.5781 - accuracy: 0.8050\n",
            "Epoch 95/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.5657 - accuracy: 0.8200\n",
            "Epoch 96/100\n",
            "20/20 [==============================] - 4s 204ms/step - loss: 0.4552 - accuracy: 0.8350\n",
            "Epoch 97/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.4880 - accuracy: 0.8300\n",
            "Epoch 98/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.5107 - accuracy: 0.8350\n",
            "Epoch 99/100\n",
            "20/20 [==============================] - 4s 201ms/step - loss: 0.4948 - accuracy: 0.8250\n",
            "Epoch 100/100\n",
            "20/20 [==============================] - 4s 203ms/step - loss: 0.4122 - accuracy: 0.8300\n",
            "1/1 [==============================] - 0s 333ms/step\n",
            "\n",
            "PREDICTION DES CARACTERISTIQUES A PARTIR D'1 IMAGE : \n",
            "numéro de l'image :  8.0 _ 80.0\n",
            "label à prédire :  8.0\n",
            "classe prédite :  [1]\n",
            "Malheureusement la prédiction est incorrecte...\n",
            "\n",
            "PREDICTION DES CARACTERISTIQUES A PARTIR DE L'ENSEMBLE DE TEST : \n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "Le pourcentage de good prédictions sur le testing set est de :  16.0 %. \n",
            " -------------------------------------------------------------------\n",
            "Le pourcentage de good prédictions pour la classe  1  est de :  20.0 %.\n",
            "Le pourcentage de good prédictions pour la classe  2  est de :  40.0 %.\n",
            "Le pourcentage de good prédictions pour la classe  3  est de :  20.0 %.\n",
            "Le pourcentage de good prédictions pour la classe  4  est de :  10.0 %.\n",
            "Le pourcentage de good prédictions pour la classe  5  est de :  0.0 %.\n",
            "Le pourcentage de good prédictions pour la classe  6  est de :  20.0 %.\n",
            "Le pourcentage de good prédictions pour la classe  7  est de :  20.0 %.\n",
            "Le pourcentage de good prédictions pour la classe  8  est de :  20.0 %.\n",
            "Le pourcentage de good prédictions pour la classe  9  est de :  0.0 %.\n",
            "Le pourcentage de good prédictions pour la classe  10  est de :  10.0 %.\n",
            "Predicted  1.0   2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0   10.0\n",
            "Reality                                                              \n",
            "1.0           2     1     4     0     1     0     0     2     0     0\n",
            "2.0           1     4     2     0     1     0     0     1     1     0\n",
            "3.0           1     2     2     1     2     0     0     2     0     0\n",
            "4.0           2     1     1     1     1     0     1     0     1     2\n",
            "5.0           0     0     6     0     0     0     0     2     1     1\n",
            "6.0           0     0     0     2     1     2     1     1     0     3\n",
            "7.0           2     1     1     0     1     0     2     3     0     0\n",
            "8.0           1     1     2     1     0     0     1     2     1     1\n",
            "9.0           0     2     0     2     0     0     1     3     0     2\n",
            "10.0          0     3     1     1     3     0     1     0     0     1\n",
            "84 grille(s) mal classée(s) :\n",
            "   1.0_71.0\n",
            "   1.0_72.0\n",
            "   1.0_73.0\n",
            "   1.0_74.0\n",
            "   1.0_75.0\n",
            "   1.0_78.0\n",
            "   1.0_79.0\n",
            "   1.0_80.0\n",
            "   2.0_71.0\n",
            "   2.0_72.0\n",
            "   2.0_76.0\n",
            "   2.0_77.0\n",
            "   2.0_79.0\n",
            "   2.0_80.0\n",
            "   3.0_72.0\n",
            "   3.0_73.0\n",
            "   3.0_75.0\n",
            "   3.0_76.0\n",
            "   3.0_77.0\n",
            "   3.0_78.0\n",
            "   3.0_79.0\n",
            "   3.0_80.0\n",
            "   4.0_71.0\n",
            "   4.0_72.0\n",
            "   4.0_73.0\n",
            "   4.0_74.0\n",
            "   4.0_75.0\n",
            "   4.0_76.0\n",
            "   4.0_77.0\n",
            "   4.0_78.0\n",
            "   4.0_79.0\n",
            "   5.0_71.0\n",
            "   5.0_72.0\n",
            "   5.0_73.0\n",
            "   5.0_74.0\n",
            "   5.0_75.0\n",
            "   5.0_76.0\n",
            "   5.0_77.0\n",
            "   5.0_78.0\n",
            "   5.0_79.0\n",
            "   5.0_80.0\n",
            "   6.0_71.0\n",
            "   6.0_72.0\n",
            "   6.0_74.0\n",
            "   6.0_75.0\n",
            "   6.0_76.0\n",
            "   6.0_77.0\n",
            "   6.0_78.0\n",
            "   6.0_79.0\n",
            "   7.0_71.0\n",
            "   7.0_72.0\n",
            "   7.0_73.0\n",
            "   7.0_74.0\n",
            "   7.0_76.0\n",
            "   7.0_77.0\n",
            "   7.0_79.0\n",
            "   7.0_80.0\n",
            "   8.0_71.0\n",
            "   8.0_72.0\n",
            "   8.0_74.0\n",
            "   8.0_75.0\n",
            "   8.0_76.0\n",
            "   8.0_77.0\n",
            "   8.0_78.0\n",
            "   8.0_80.0\n",
            "   9.0_71.0\n",
            "   9.0_72.0\n",
            "   9.0_73.0\n",
            "   9.0_74.0\n",
            "   9.0_75.0\n",
            "   9.0_76.0\n",
            "   9.0_77.0\n",
            "   9.0_78.0\n",
            "   9.0_79.0\n",
            "   9.0_80.0\n",
            "   10.0_72.0\n",
            "   10.0_73.0\n",
            "   10.0_74.0\n",
            "   10.0_75.0\n",
            "   10.0_76.0\n",
            "   10.0_77.0\n",
            "   10.0_78.0\n",
            "   10.0_79.0\n",
            "   10.0_80.0\n",
            "MAE :  2.85\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n-------------\\nBONJOUR, JE SUIS GULDURN et  ! \\nJE SUIS UN cnn QUI VA ESTIMER LES CARACTERISTIQUES DE PLANTES A PARTIR D'IMAGES.\\n-------------\\n\")\n",
        "\n",
        "# IMPORTS -------------------------------------------------------------------------------------------------------\n",
        "print(\"\\nIMPORTS ...\")\n",
        "import random\n",
        "\n",
        "# DATASET -------------------------------------------------------------------------------------------------------\n",
        "print(\"\\nLIENS AVEC LES DATASETS\")\n",
        "TRAINING_FILE = \"./training.csv\"\n",
        "TESTING_FILE = \"./testing.csv\"\n",
        "\n",
        "# PARSING -------------------------------------------------------------------------------------------------------\n",
        "print(\"\\nPARSING ... \")\n",
        "training_images, training_labels = parse_data_from_input(TRAINING_FILE, training_size, training_longueur, largeur_image, longueur_image, nb_pixels_images, n_classes)\n",
        "print(f\"Training images has shape: {training_images.shape} and dtype: {training_images.dtype}\")\n",
        "print(f\"Training labels has shape: {training_labels.shape} and dtype: {training_labels.dtype}\")\n",
        "\n",
        "\n",
        "# DATA AUGMENTATION -------------------------------------------------------------------------------------------------------\n",
        "print(\"\\nDATA AUGMENTATION\")\n",
        "train_generator = simple_train_val_generators(training_images, training_labels, training_size, largeur_image, longueur_image, nb_channel, n_classes)\n",
        "print(f\"Images of training generator have shape: {train_generator.x.shape}\")\n",
        "print(f\"Labels of training generator have shape: {train_generator.y.shape}\")\n",
        "\n",
        "\n",
        "# RESEAU -------------------------------------------------------------------------------------------------------\n",
        "print(\"\\nCNN\")\n",
        "model = run_model(train_generator, n_classes, largeur_image, longueur_image, nb_channel, 1)\n",
        "\n",
        "\n",
        "# PREDICTIONS -------------------------------------------------------------------------------------------------------\n",
        "# prédiction sur 1 image de test\n",
        "cas, label, classe, img, correct = prediction_1_vecteur(TESTING_FILE, largeur_image, longueur_image, nb_channel, random.randint(0,(n_classes-1)), random.randint(0,(testing_size-1)), nb_pixels_images, model, n_classes, 1)\n",
        "# prédiction sur l'ensemble de test\n",
        "tab_good_pred_per_classe = prediction_dataset(TESTING_FILE, testing_size, nb_pixels_images, model, n_classes, 0)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
