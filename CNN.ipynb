{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be5uBRdgbhey"
      },
      "source": [
        "# CNNs avec Keras sur des données de paysage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-diCsctWfSh"
      },
      "source": [
        "## Vérification de l'utilisation de GPU et import dataset\n",
        "\n",
        "Allez dans le menu `Exécution > Modifier le type d'execution` et vérifiez que l'on est bien en Python 3 et que l'accélérateur matériel est configuré sur « GPU »."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b dev-rose https://github.com/Finweto/Projet-deep-learning-2022.git\n",
        "!ls "
      ],
      "metadata": {
        "id": "kvV1ShG14BL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S9vf7TtjjMa"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5hheTPDcdqN"
      },
      "source": [
        "## Import des librairies nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvZJ4yH6cNHE"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import typing\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "import sklearn.utils\n",
        "import sklearn.metrics\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zCSZcuhczAn"
      },
      "source": [
        "## Préparation des données\n",
        "\n",
        "\n",
        "Après avoir chargé chaque image, nous allons passer leur canaux en RGB puis les redimensionner à 150x150, enfin, par défaut, nous retournerons un dataset mélangé grâce à [`sklearn.utils.shuffle`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kv62rzC3wIO"
      },
      "outputs": [],
      "source": [
        "label_names = [\"convolvulaceae\",\"monimiaceae\",\"amborella\",\"castanea\",\"desmodium\",\"eugenia\",\n",
        "              \"laurus\",\"litsea\",\"magnolia\",\"rubus\",\"ulmus\"]\n",
        "\n",
        "def get_images(filename):\n",
        "\n",
        "  df=pd.read_csv(filename)\n",
        "  df=df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  images = []\n",
        "  labels = []\n",
        "\n",
        "  for i in range(len(df[\"label\"])):\n",
        "    path=\"Projet-deep-learning-2022/\"+df[\"img_path\"][i]\n",
        "\n",
        "    image=cv2.imread(path)\n",
        "    image=cv2.resize(image,(150,150))\n",
        "    images.append(image)\n",
        "\n",
        "    label=df[\"label\"][i]\n",
        "    labels.append(label)\n",
        "\n",
        "  return images,labels\n",
        "\n",
        "label_to_index = {l: i for i, l in enumerate(label_names)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Co3AuagRdQ"
      },
      "source": [
        "## Appel à `get_images`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlWHQMqbgUi_"
      },
      "outputs": [],
      "source": [
        "images, labels= get_images(\"Projet-deep-learning-2022/data_train_labeled.csv\")\n",
        "images_test,labels_test=get_images(\"Projet-deep-learning-2022/data_test_labeled.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation split"
      ],
      "metadata": {
        "id": "TJu4oy5ixMDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "images, images_validation, labels, labels_validation = train_test_split(images, labels, stratify=labels, test_size=0.3) # before model building"
      ],
      "metadata": {
        "id": "7-sn3sNVDgcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Affichage des labels"
      ],
      "metadata": {
        "id": "qbTMr1lsxRkN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9CFPgYahZ-_"
      },
      "outputs": [],
      "source": [
        "seaborn.countplot(x=labels)\n",
        "plt.title(\"Décomptes des différents labels\")\n",
        "plt.ylabel(\"Décompte\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Affichage d'exemples d'images"
      ],
      "metadata": {
        "id": "21OiPWY49Bph"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRP8sepdhh8I"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# generation d'index aleatoires\n",
        "random_indexes = []\n",
        "for i in range(15):\n",
        "  random_index = random.randint(0,20)\n",
        "  if (random_index not in random_indexes):\n",
        "    random_indexes.append(random_index)\n",
        "\n",
        "# affichage d'exemples d'images et de leur label\n",
        "\n",
        "for i in range(1,len(random_indexes)):\n",
        "  plt.figure()\n",
        "  random_index= random_indexes[i]\n",
        "  plt.imshow(images[random_index])\n",
        "  plt.title('Exemple ['+str(random_index)+'] '+labels[random_index])\n",
        "  plt.axis('off')\n",
        " \n",
        "plt.show() \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation (TO DO work in progress nathan)"
      ],
      "metadata": {
        "id": "kkdsw8k49aTh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbzuJrDl3_T6"
      },
      "source": [
        "#### Pré traitement de la data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4equd-uY3_T7"
      },
      "outputs": [],
      "source": [
        "images=np.array(images)\n",
        "labels=np.array(labels)\n",
        "\n",
        "images_test=np.array(images_test)\n",
        "labels_test=np.array(labels_test)\n",
        "\n",
        "images_validation=np.array(images_validation)\n",
        "labels_validation=np.array(labels_validation)\n",
        "\n",
        "labels_int=[label_names.index(x) for x in labels]\n",
        "labels_int=np.array(labels_int)\n",
        "\n",
        "labels_test_int=[label_names.index(x) for x in labels_test]\n",
        "labels_test_int=np.array(labels_test_int)\n",
        "\n",
        "labels_validation_int=[label_names.index(x) for x in labels_validation]\n",
        "labels_validation_int=np.array(labels_validation_int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def vertical_shift(images):\n",
        "  # creation image data augmentation generator width_shift (vertical shift)\n",
        "  datagen = ImageDataGenerator(width_shift_range=0.2)\n",
        "  # creation of the images\n",
        "  vertical_shift_images = datagen.flow(images, batch_size=1)\n",
        "\n",
        "  return vertical_shift_images\n",
        "\n",
        "def horizontal_shift(images):\n",
        "  # creation image data augmentation generator height_shift (horizontal shift)\n",
        "  datagen = ImageDataGenerator(height_shift_range=0.2)\n",
        "\n",
        "  # creation of the images\n",
        "  horizontal_shift_images = datagen.flow(images,batch_size=1)\n",
        "\n",
        "  return horizontal_shift_images\n",
        "\n",
        "def horizontal_flip(images):\n",
        "  # creation image data augmentation generator horizontal_flip\n",
        "  datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "  # creation of the images\n",
        "  horizontal_flip_images = datagen.flow(images,batch_size=1)\n",
        "\n",
        "  return horizontal_flip_images\n",
        "\n",
        "def vertical_flip(images):\n",
        "  # creation image data augmentation generator vertical_flip\n",
        "  datagen = ImageDataGenerator(vertical_flip=True)\n",
        "\n",
        "  # creation of the images\n",
        "  vertical_flip_images = datagen.flow(images,batch_size=1)\n",
        "\n",
        "  return vertical_flip_images\n",
        "\n",
        "def rotation_change(images):\n",
        "  # creation image data augmentation generator rotation_change\n",
        "  datagen = ImageDataGenerator(rotation_range=50)\n",
        "\n",
        "  # creation of the images\n",
        "  rotation_images = datagen.flow(images,batch_size=1)\n",
        "\n",
        "  return rotation_images\n",
        "\n",
        "def brightness_change(images):\n",
        "  # creation image data augmentation generator brightness_change\n",
        "  datagen = ImageDataGenerator(brightness_range=[0.15,2.0])\n",
        "\n",
        "  # creation of the images\n",
        "  brightness_images = datagen.flow(images, batch_size=1)\n",
        "\n",
        "  return brightness_images\n",
        "  "
      ],
      "metadata": {
        "id": "NKrzYl4lptoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fonction taking our images and labels, returning modified images with corresponding labels for our lack of data\n",
        "def imageAugmentation(images):\n",
        "  augmented_images = []\n",
        "  temp = []\n",
        "\n",
        "  temp.append(vertical_shift(images))\n",
        "  #temp.append(horizontal_shift(images))\n",
        "  #temp.append(horizontal_flip(images))\n",
        "  #temp.append(vertical_flip(images))\n",
        "  #temp.append(rotation_change(images))\n",
        "  #temp.append(brightness_change(images))\n",
        "\n",
        "  for results in temp:\n",
        "    for elem in results:\n",
        "      augmented_images.append(elem)\n",
        "\n",
        "  return augmented_images\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_f6pwwGmzY05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_images = imageAugmentation(images)\n"
      ],
      "metadata": {
        "id": "CxPsAO1YmwUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for augmented_image in augmented_images:\n",
        "  np.append(images, augmented_image)\n",
        "\n",
        "labels = np.concatenate((labels,labels))"
      ],
      "metadata": {
        "id": "Knhw9efUDO5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab = np.array([1,2,3])\n",
        "tab = tab*2"
      ],
      "metadata": {
        "id": "S246zohXDlk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woNSPikeU_xl"
      },
      "source": [
        "## Apprentissage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics"
      ],
      "metadata": {
        "id": "9koM3XVP9hAd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-j7dwcWUdZx"
      },
      "outputs": [],
      "source": [
        "# Plot des métriques d'entraînement\n",
        "def plot_metrics(history) -> None:\n",
        "  plt.plot(training.history[\"acc\"])\n",
        "  #plt.plot(training.history[\"val_accuracy\"])\n",
        "  plt.title(\"Accuracy du modèle\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.legend([\"Entraînement\", \"Validation\"], loc=\"upper left\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(training.history[\"loss\"])\n",
        "  plt.plot(training.history[\"val_loss\"])\n",
        "  plt.title(\"Perte du modèle\")\n",
        "  plt.ylabel(\"Perte\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.legend([\"Entraînement\", \"Validation\"], loc=\"upper right\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modèle 1"
      ],
      "metadata": {
        "id": "QJZI304F9j6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers as layer\n",
        "# Le modèle\n",
        "model = keras.models.Sequential()\n",
        "model.add(layer.Conv2D(128,3,activation='relu',input_shape=(200,200,3)))\n",
        "model.add(layer.Conv2D(64,3,activation='relu'))\n",
        "model.add(layer.Conv2D(32,3,activation='relu'))\n",
        "model.add(layer.MaxPooling2D(3))\n",
        "model.add(layer.Dropout(0.2))\n",
        "\n",
        "model.add(layer.Conv2D(128,3,activation='relu'))\n",
        "model.add(layer.Conv2D(64,3,activation='relu'))\n",
        "model.add(layer.Conv2D(32,3,activation='relu'))\n",
        "model.add(layer.MaxPooling2D(3))\n",
        "model.add(layer.Dropout(0.2))\n",
        "\n",
        "model.add(layer.Conv2D(128,3,activation='relu'))\n",
        "model.add(layer.Conv2D(64,3,activation='relu'))\n",
        "model.add(layer.Conv2D(32,3,activation='relu'))\n",
        "model.add(layer.MaxPooling2D(3))\n",
        "model.add(layer.Dropout(0.2))\n",
        "\n",
        "model.add(layer.Flatten())\n",
        "\n",
        "model.add(layer.Dense(512, activation='relu'))\n",
        "model.add(layer.Dropout(0.4))\n",
        "\n",
        "model.add(layer.Dense(11, activation=\"softmax\"))\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=['acc'],\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        ")\n",
        "\n",
        "# Sommaire du modèle\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "FMXHIhPnn9Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entrainement du modèle"
      ],
      "metadata": {
        "id": "dDV0s3oW924R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apprentissage du modèle\n",
        "training = model.fit(images,\n",
        "                     labels_int,\n",
        "                     epochs=20,\n",
        "                     validation_split=0.30,\n",
        "                     #validation_data=(images_validation, labels_validation_int),\n",
        "                     batch_size=8,\n",
        "                     shuffle=True)\n",
        "\n",
        "# Visualisation des métriques d'entrainement\n",
        "plot_metrics(training.history)"
      ],
      "metadata": {
        "id": "-dXuwTlImkZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb_IT9QWeYPh"
      },
      "source": [
        "## Évaluation des performances sur l'ensemble de test\n",
        "\n",
        "Dans le dossier `seg_test` se trouve un ensemble de données qui n'ont jamais été vues durant l'apprentissage.\n",
        "\n",
        "On utilisera la méthode `evaluate(X, y)` du modèle pour évaluer la qualité de nos prédictions sur ce dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJubx-J7i882"
      },
      "outputs": [],
      "source": [
        "model.evaluate(images_test, labels_test_int, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYPaPc0lpe0B"
      },
      "source": [
        "## Analyse d'erreur\n",
        "\n",
        "On affiche la matrice de confusion puis on regarde des images mal classées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfOlRq1Ep3KD"
      },
      "outputs": [],
      "source": [
        "def analyze_preds(preds, labels):\n",
        "  confusion_matrix = sklearn.metrics.confusion_matrix(preds,\n",
        "                                                      labels,\n",
        "                                                      normalize=\"true\")\n",
        "  seaborn.heatmap(confusion_matrix,\n",
        "                  cmap=\"rocket_r\",\n",
        "                  xticklabels=label_names,\n",
        "                  yticklabels=label_names)\n",
        "  plt.title(\"Matrice de confusion\")\n",
        "  plt.show()\n",
        "\n",
        "  seaborn.countplot(x=list(map(lambda x: label_names[x], preds)))\n",
        "  plt.title(\"Décomptes des classes prédites\")\n",
        "  plt.ylabel(\"Décompte\")\n",
        "  plt.xlabel(\"Class\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "test_pred = np.argmax(model.predict(images_test), axis=-1)\n",
        "analyze_preds(test_pred, labels_test_int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suite de l'analyze (TO DO)"
      ],
      "metadata": {
        "id": "dNb2Xzn9-adA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLUCp97BeUxu"
      },
      "outputs": [],
      "source": [
        "def plot_mistakes(predicted_class: str, true_class: str) -> None:\n",
        "  print(f\"Prédiction : {predicted_class}, classe réelle : {true_class}\")\n",
        "  mistakes = images_test[(test_pred == label_to_index[predicted_class])\n",
        "                         & (labels_test_int == label_to_index[true_class])]\n",
        "  random_indexes = np.random.choice(mistakes.shape[0],\n",
        "                                       size=min(mistakes.shape[0], 25),\n",
        "                                       replace=False)\n",
        "  grid_indexes = itertools.product(range(5), repeat=2)\n",
        "\n",
        "  _, ax = plt.subplots(5, 5, figsize=(15, 15))\n",
        "  for img_index, (i, j) in zip(random_indexes, grid_indexes):\n",
        "    ax[i, j].imshow(mistakes[img_index])\n",
        "    ax[i, j].axis(\"off\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43kc6Wgd9bkC"
      },
      "outputs": [],
      "source": [
        "# Plot les images prédites glacier alors qu'elles ont un label montagne\n",
        "plot_mistakes(\"glacier\", \"mountain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QyUe_6T_vL8"
      },
      "outputs": [],
      "source": [
        "# Plot les images prédites glacier alors qu'elles ont un label mer\n",
        "plot_mistakes(\"glacier\", \"sea\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAgJItMSDE2-"
      },
      "outputs": [],
      "source": [
        "# Plot les images prédites bâtiment alors qu'elles ont un label mer\n",
        "plot_mistakes(\"buildings\", \"sea\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VrH5FRb_OBy"
      },
      "source": [
        "## Transfert d'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCDC0Hw6_Qrn"
      },
      "outputs": [],
      "source": [
        "base_model = keras.applications.EfficientNetB7(include_top=False,\n",
        "                                               weights=\"imagenet\",\n",
        "                                               input_shape=(150, 150, 3))\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [base_model,\n",
        "     keras.layers.SpatialDropout2D(0.1),\n",
        "     keras.layers.Conv2D(1024, 1, activation=\"relu\"),\n",
        "     keras.layers.SpatialDropout2D(0.1),\n",
        "     keras.layers.Conv2D(256, 1, activation=\"relu\"),\n",
        "     keras.layers.SpatialDropout2D(0.1),\n",
        "     keras.layers.Conv2D(64, 1, activation=\"relu\"),\n",
        "     keras.layers.SpatialDropout2D(0.1),\n",
        "     keras.layers.Conv2D(16, 1, activation=\"relu\"),\n",
        "     keras.layers.SpatialDropout2D(0.1),\n",
        "     keras.layers.Flatten(),\n",
        "     keras.layers.Dense(6, activation=\"softmax\", kernel_regularizer=\"l2\")])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzYvqXA6A4_b"
      },
      "outputs": [],
      "source": [
        "training = model.fit(images,\n",
        "                     labels,\n",
        "                     epochs=5,\n",
        "                     validation_split=0.30,\n",
        "                     batch_size=512)\n",
        "\n",
        "plot_metrics(training.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W-qBnuTAegR"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_images, test_labels, verbose=1)\n",
        "test_preds = numpy.argmax(model.predict(test_images), axis=-1)\n",
        "analyze_preds(test_preds, test_labels)\n",
        "plot_mistakes(\"glacier\", \"mountain\")\n",
        "plot_mistakes(\"glacier\", \"sea\")\n",
        "plot_mistakes(\"buildings\", \"sea\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Q5NiBKjO6X"
      },
      "source": [
        "## Prédire dans des condition « réelles »\n",
        "\n",
        "Dans le dossier `seg_pred` se trouvent des images non-annotées. On ne peut donc pas évaluer correctement les performances sur cet ensemble. \n",
        "\n",
        "Cependant, on peut afficher des photos et les probabilités que notre modèle attribue à chaque classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yPf4WcytFzQ"
      },
      "outputs": [],
      "source": [
        "pred_images, _, pred_file_paths = get_images(\n",
        "    pathlib.Path(\"dataset-landscape\") / \"seg_pred\")\n",
        "pred_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMLmuIqvWAS6"
      },
      "outputs": [],
      "source": [
        "# Création de la grille de sous-plots. On donne l'argument figsize pour agrandir\n",
        "# la taille de la figure qui est petite par défaut\n",
        "_, ax = plt.subplots(10, 5, figsize=(30, 45))\n",
        "\n",
        "# On choisit 25 indices au hasard, sans replacement (on ne veut pas afficher la\n",
        "# même image deux fois)\n",
        "random_indexes = numpy.random.choice(pred_images.shape[0],\n",
        "                                     size=(5, 5),\n",
        "                                     replace=False)\n",
        "\n",
        "for i in range(5):\n",
        "  for j in range(5):\n",
        "    img_index = random_indexes[i, j]\n",
        "    # Récupération de l'image et prédiction de sa classe\n",
        "    image = pred_images[img_index]\n",
        "    probabilities = model.predict(image[None, ...])[0]\n",
        "    predicted_class = label_names[numpy.argmax(probabilities)]\n",
        "\n",
        "    # Affichage avec matplotlib et sa fonction imshow, très pratique en vision\n",
        "    # par ordinateur\n",
        "    ax[i * 2, j].imshow(image)\n",
        "    ax[i * 2, j].set_title(f\"Exemple {img_index}\")\n",
        "    ax[i * 2, j].axis('off')\n",
        "\n",
        "    # Affichage de la distribution de prédiction sur la ligne d'en dessous\n",
        "    ax[i * 2 + 1, j].bar(label_names, probabilities)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "hb_IT9QWeYPh",
        "RYPaPc0lpe0B",
        "5VrH5FRb_OBy",
        "v4Q5NiBKjO6X"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}